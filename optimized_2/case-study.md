# Case-study оптимизации

## Актуальная проблема
В нашем проекте возникла серьёзная проблема.

Необходимо было обработать файл с данными, чуть больше ста мегабайт.

У нас уже была программа на `ruby`, которая умела делать нужную обработку.

Она успешно работала на файлах размером пару мегабайт, но для большого файла она
работала слишком долго, и не было понятно, закончит ли она вообще работу за
какое-то разумное время.

Я решил исправить эту проблему, оптимизировав эту программу.

## Формирование метрики
Для того, чтобы понимать, дают ли мои изменения положительный эффект на
быстродействие программы я придумал использовать такую метрику:

*Кол-во итераций в секунду (isp) выполнения программы на данных размером 0.25Mb.*

## Анализ асимптотики
Постараюсь оценить сколько времени займет обработка большого файла.

Для этого соберу данные на файлах размера x, 2x, 4x, 8x, ... и попробую понять закономерность.

Для сбора данных для оценки асимптотики сделал отдельный скрипт `asymptotics.rb`

Вот его вывод для исходной версии программы:

```
Calculating -------------------------------------
    Process 0.0625Mb      8.359  (± 0.0%) i/s -     42.000  in   5.041685s
     Process 0.125Mb      2.542  (± 0.0%) i/s -     13.000  in   5.123614s
      Process 0.25Mb      0.631  (± 0.0%) i/s -      4.000  in   6.362890s
       Process 0.5Mb      0.095  (± 0.0%) i/s -      1.000  in  10.545360s
         Process 1Mb      0.023  (± 0.0%) i/s -      1.000  in  43.283274s
         Process 2Mb      0.005  (± 0.0%) i/s -      1.000  in 193.212175s

Comparison:
    Process 0.0625Mb:        8.4 i/s
     Process 0.125Mb:        2.5 i/s - 3.29x     slower # x 
      Process 0.25Mb:        0.6 i/s - 13.25x    slower # x * 4
       Process 0.5Mb:        0.1 i/s - 88.15x    slower # x * 6.6
         Process 1Mb:        0.0 i/s - 361.82x   slower # x * 4.1
         Process 2Mb:        0.0 i/s - 1615.12x  slower # x * 4.4
```

Просматривается такая тенденция: при увеличении объёма исходных данных в два раза,
время замедляется в ~5 раз!

Собрал данные в [таблицу](https://docs.google.com/spreadsheets/d/1GS4ckDQJTEhd9ZTHj1PPxtEfD9fsyQV2uLgCabZDlKU/edit?usp=sharing)

Получилась такая асимптотика: `5ˆ(log2(size))`

Функция растет очень резко, надо разбираться со сложностью алгоритма.

## Гарантия корректности работы оптимизированной программы
Программа поставлялась с тестом. Выполнение этого теста позволяет не допустить
изменения логики программы при оптимизации.

## Feedback-Loop
Для того, чтобы иметь возможность быстро проверять гипотезы я выстроил
эффективный `feedback-loop`, который позволил мне получать обратную связь по
эффективности сделанных изменений за 5 секунд.

Я написал отдельный скрипт `feedback-loop.rb`, который:
- выполняет замер метрики
- проверяет прохождение теста
- выводит результаты на экран

Используя его я могу быстро тестировать гипотезы и понимать их влияние на целевую
метрику. При этом выполнение теста защищает меня от случайной регрессии
функциональности системы на каждом шаге.

При этом саму оптимизацию программы я вынес в отдельный файл `optimized_2.rb`.

## Фиксируем исходное состояние системы
Исходная программа имеет метрику:
**~0.643 ips**

```
Calculating -------------------------------------
Process 0.25 MB of data
  0.643  (± 0.0%) i/s - 4.000  in   6.245582s
```

## Вникаем в детали системы, чтобы найти 20% точек роста

### Находка №1
Асимптотический анализ показал, что сложность программы растет пропорционально `5ˆ(log2(size))`

Думаю, это основная проблема. Надо поискать с помощью профилировщика, в каком месте расходуется наибольшее кол-ва времени.

Профилирую программу с помощью **stackprof** в режиме **wall**

Для удобства заведу еще один файл, `stackp.rb`, с помощью которого буду
выполнять профилировку **stackprof**.

Отчет *stackprof* показывает, что 93.0% времени выполняется строка

```
 10542   (93.0%)                    |    96  |   users.each do |user|
                                    |    97  |     attributes = user
 20960  (184.8%) /  10480  (92.4%)  |    98  |     user_sessions = sessions.select { |session| session['user_id'] == user['id'] }
   34    (0.3%)                     |    99  |     user_object = User.new(attributes: attributes, sessions: user_sessions)
   28    (0.2%) /    28   (0.2%)    |   100  |     users_objects = users_objects + [user_object]
                                    |   101  |   end
```

Мы в цикле обходим всех юзеров и для каждого из них выполняем полное
сканирование массива сессий! Это от части объясняет асимптотику исходной программы.

Для решения этой проблемы заполним хэш вида `user_sessions = {user_id: user_sessions}`,
чтобы потом не искать сессии пользователя по массиву.

#### Эффект изменения
Метрика выросла с `0.643ips` до `2.952ips`, то есть в 4.5 раз

```
Calculating -------------------------------------
Process 0.25 MB of data
  2.952  (± 0.0%) i/s - 15.000  in   5.110584s
```

Пересчитаем асимпторику!

```
Calculating -------------------------------------
    Process 0.0625Mb     13.346  (±15.0%) i/s -     66.000  in   5.029983s
     Process 0.125Mb      5.077  (±19.7%) i/s -     25.000  in   5.090121s
      Process 0.25Mb      2.571  (± 0.0%) i/s -     13.000  in   5.131196s
       Process 0.5Mb      1.266  (± 0.0%) i/s -      7.000  in   5.553357s
         Process 1Mb      0.443  (± 0.0%) i/s -      3.000  in   6.881357s
         Process 2Mb      0.114  (± 0.0%) i/s -      1.000  in   8.792635s

Comparison:
    Process 0.0625Mb:       13.3 i/s
     Process 0.125Mb:        5.1 i/s - 2.63x    slower # x
      Process 0.25Mb:        2.6 i/s - 5.19x    slower # x * 1.97
       Process 0.5Mb:        1.3 i/s - 10.54x   slower # x * 2
         Process 1Mb:        0.4 i/s - 30.15x   slower # x * 2.86
         Process 2Mb:        0.1 i/s - 117.35x  slower # x * 3.89
```

Что ж, похоже, теперь при росте данных в два раза время растет в ~3 раза, а не в 5.

Уже лучше, коммичу первое изменение на этом.
