# Case-study оптимизации

## Актуальная проблема
В нашем проекте возникла серьёзная проблема.

Необходимо было обработать файл с данными, чуть больше ста мегабайт.

У нас уже была программа на `ruby`, которая умела делать нужную обработку.

Она успешно работала на файлах размером пару мегабайт, но для большого файла она
работала слишком долго, и не было понятно, закончит ли она вообще работу за
какое-то разумное время.

Я решил исправить эту проблему, оптимизировав эту программу.

## Формирование метрики
Для того, чтобы понимать, дают ли мои изменения положительный эффект на
быстродействие программы я придумал использовать такую метрику:

*Кол-во итераций в секунду (isp) выполнения программы на данных размером 0.25Mb.*

## Анализ асимптотики до оптимизации
Постараюсь оценить сколько времени займет обработка большого файла.

Для этого соберу данные на файлах размера x, 2x, 4x, 8x, ... и попробую понять закономерность.

Для сбора данных для оценки асимптотики сделал отдельный скрипт `asymptotics.rb`

Вот его вывод для исходной версии программы:

```
Calculating -------------------------------------
    Process 0.0625Mb      8.359  (± 0.0%) i/s -     42.000  in   5.041685s
     Process 0.125Mb      2.542  (± 0.0%) i/s -     13.000  in   5.123614s
      Process 0.25Mb      0.631  (± 0.0%) i/s -      4.000  in   6.362890s
       Process 0.5Mb      0.095  (± 0.0%) i/s -      1.000  in  10.545360s
         Process 1Mb      0.023  (± 0.0%) i/s -      1.000  in  43.283274s
         Process 2Mb      0.005  (± 0.0%) i/s -      1.000  in 193.212175s

Comparison:
    Process 0.0625Mb:        8.4 i/s
     Process 0.125Mb:        2.5 i/s - 3.29x     slower # x 
      Process 0.25Mb:        0.6 i/s - 13.25x    slower # x * 4
       Process 0.5Mb:        0.1 i/s - 88.15x    slower # x * 6.6
         Process 1Mb:        0.0 i/s - 361.82x   slower # x * 4.1
         Process 2Mb:        0.0 i/s - 1615.12x  slower # x * 4.4
```

Просматривается такая тенденция: при увеличении объёма исходных данных в два раза,
время замедляется в ~5 раз!

Собрал данные в [таблицу](https://docs.google.com/spreadsheets/d/1GS4ckDQJTEhd9ZTHj1PPxtEfD9fsyQV2uLgCabZDlKU/edit?usp=sharing)

Получилась такая асимптотика: `5ˆ(log2(size))`

Функция растет очень резко, надо разбираться со сложностью алгоритма.

## Гарантия корректности работы оптимизированной программы
Программа поставлялась с тестом. Выполнение этого теста позволяет не допустить
изменения логики программы при оптимизации.

## Feedback-Loop
Для того, чтобы иметь возможность быстро проверять гипотезы я выстроил
эффективный `feedback-loop`, который позволил мне получать обратную связь по
эффективности сделанных изменений за 5 секунд.

Я написал отдельный скрипт `feedback-loop.rb`, который:
- выполняет замер метрики
- проверяет прохождение теста
- выводит результаты на экран

Используя его я могу быстро тестировать гипотезы и понимать их влияние на целевую
метрику. При этом выполнение теста защищает меня от случайной регрессии
функциональности системы на каждом шаге.

При этом саму оптимизацию программы я вынес в отдельный файл `optimized_2.rb`.

## Фиксируем исходное состояние системы
Исходная программа имеет метрику:
**~0.643 ips**

```
Calculating -------------------------------------
Process 0.25 MB of data
  0.643  (± 0.0%) i/s - 4.000  in   6.245582s
```

## Вникаем в детали системы, чтобы найти 20% точек роста

### Находка №1
Асимптотический анализ показал, что сложность программы растет пропорционально `5ˆ(log2(size))`

Думаю, это основная проблема. Надо поискать с помощью профилировщика, в каком месте расходуется наибольшее кол-ва времени.

Профилирую программу с помощью **stackprof** в режиме **wall**

Для удобства заведу еще один файл, `stackp.rb`, с помощью которого буду
выполнять профилировку **stackprof**.

Отчет *stackprof* показывает, что 93.0% времени выполняется строка

```
 10542   (93.0%)                    |    96  |   users.each do |user|
                                    |    97  |     attributes = user
 20960  (184.8%) /  10480  (92.4%)  |    98  |     user_sessions = sessions.select { |session| session['user_id'] == user['id'] }
   34    (0.3%)                     |    99  |     user_object = User.new(attributes: attributes, sessions: user_sessions)
   28    (0.2%) /    28   (0.2%)    |   100  |     users_objects = users_objects + [user_object]
                                    |   101  |   end
```

Мы в цикле обходим всех юзеров и для каждого из них выполняем полное
сканирование массива сессий! Это от части объясняет асимптотику исходной программы.

Для решения этой проблемы заполним хэш вида `user_sessions = {user_id: user_sessions}`,
чтобы потом не искать сессии пользователя по массиву.

#### Эффект изменения
Метрика выросла с `0.643ips` до `2.952ips`, то есть в 4.5 раз

```
Calculating -------------------------------------
Process 0.25 MB of data
  2.952  (± 0.0%) i/s - 15.000  in   5.110584s
```

Пересчитаем асимпторику!

```
Calculating -------------------------------------
    Process 0.0625Mb     13.346  (±15.0%) i/s -     66.000  in   5.029983s
     Process 0.125Mb      5.077  (±19.7%) i/s -     25.000  in   5.090121s
      Process 0.25Mb      2.571  (± 0.0%) i/s -     13.000  in   5.131196s
       Process 0.5Mb      1.266  (± 0.0%) i/s -      7.000  in   5.553357s
         Process 1Mb      0.443  (± 0.0%) i/s -      3.000  in   6.881357s
         Process 2Mb      0.114  (± 0.0%) i/s -      1.000  in   8.792635s

Comparison:
    Process 0.0625Mb:       13.3 i/s
     Process 0.125Mb:        5.1 i/s - 2.63x    slower # x
      Process 0.25Mb:        2.6 i/s - 5.19x    slower # x * 1.97
       Process 0.5Mb:        1.3 i/s - 10.54x   slower # x * 2
         Process 1Mb:        0.4 i/s - 30.15x   slower # x * 2.86
         Process 2Mb:        0.1 i/s - 117.35x  slower # x * 3.89
```

Что ж, похоже, теперь при росте данных в два раза время растет в ~3 раза, а не в 5.

Уже лучше, коммичу первое изменение на этом.

### Находка №2
Теперь попробую посмотреть отчет **memory_profiler**

Тоже заведу для удобства отдельный файл `memp.rb`. Он выполняет работу для файла размером `0.5Mb`

Отчет `memory_profiler` показывает, что больше всего памяти выделяется в строках:

```
74: sessions = sessions + [session]               # (505_035_240)
51: users_objects = users_objects + [user_object] # (16_621_440)
68: users = users + [user]                        # (16_621_440)
```

Рефакторим:

```
74: sessions << session
51: users_objects << user_object
68: users << user
```

В результате они полностью выпали из отчета.

Запустим измерение метрики.
Метрика выросла с `2.952ips` до `3.820`

```
Calculating -------------------------------------
Process 0.25 MB of data
  3.820  (± 0.0%) i/s - 20.000  in   5.256851s
```

Метрика практически не изменилась.
Очевидно, несмотря на то, что мы сделали хорошую оптимизацию по выделяемой памяти, она не сильно повлияла
на нашу основную метрику.

Коммитим и продолжаем искать.

### Находка №3
Пробую посмотреть отчет `ruby-prof` в режиме `RubyProf::WALL_TIME`

Как обычно для удобства заведу отдельный скрипт `rubyp.rb`

Отчет `ruby-prof` показывает, что `91%` всей памяти выделяется в трех вызовах
метода `Array#each` из нашего метода `work`.

Так не понятно, какой из трех вызовов самый проблемный.
Что бы стало понятнее, сделаю небольшой рефакторинг - вынесу каждый вызов `each`
из `work` в отдельный метод.

И стало понятно, что цикл для парсинга строк файла с данными, выделяет `90.30%` памяти:

```
  File.read(data_file).split("\n").each do |line|
    cols = line.split(',')
    if cols[0] == 'user'
      user = parse_user(line)
      users << user
      users_sessions[user['id']] ||= []
    end

    if cols[0] == 'session'
      session = parse_session(line)
      sessions << session
      users_sessions[session['user_id']] << session
    end
  end
```

Вопервых сразу бросается в глаза дорогая операция `File.read`.
А также `split("\n")` и `line.split(',')`.
Было принято решение использовать `File.open` и полностью отрефакторить данный цикл.

#### Эффект изменения
`ruby-prof` показывает, что `68.09%` на этот цикл.
Метрика выросла с `3.820ips` до `5.050ips`, то есть в 1.3 раза
Не много, но зато стабильно.

```
Calculating -------------------------------------
Process 0.25 MB of data
  5.050  (± 0.0%) i/s - 26.000  in   5.165178s
```
Делаем Коммит!

### Находка №4
Отчет `ruby-prof` показывает, что `66.77%` всей памяти выделяется на 7 вызовов метода `collect_stats_from_users`.
Сразу бросается в глаза, что все 7 вызовов тут излишни и хэшь можно собрать за одну итерацию.

Рефакторим
```
  users.each do |user|
    user_sessions = users_sessions.delete(user[:id]) || []
    report[:usersStats][user[:full_name]] = {
      sessionsCount: user_sessions.count,
      totalTime: user_sessions.map {|s| s[:time]}.map {|t| t.to_i}.sum.to_s + ' min.',
      longestSession: user_sessions.map {|s| s[:time]}.map {|t| t.to_i}.max.to_s + ' min.',
      browsers: user_sessions.map {|s| s[:browser]}.map {|b| b.upcase}.sort.join(', '),
      usedIE: user_sessions.map{|s| s[:browser]}.any? { |b| b.upcase =~ /INTERNET EXPLORER/ },
      alwaysUsedChrome: user_sessions.map{|s| s[:browser]}.all? { |b| b.upcase =~ /CHROME/ },
      dates: user_sessions.map{|s| s[:date]}.map {|d| Date.parse(d)}.sort.reverse.map { |d| d.iso8601 }
    }
  end
```

#### Эффект изменения
Метрика выросла с `5.050ips` до `6.179ips`

```
Calculating -------------------------------------
Process 0.25 MB of data
  6.179  (± 0.0%) i/s - 31.000  in   5.039909s
```

### Находка №5
Отчет `ruby-prof` показывает, что `31.82%` всей памяти выделяется на `<Class::Date>#parse`
Дороговато, для такой операции.
Заменяем на `Date.strptime`

#### Эффект изменения
Метрика выросла с `6.179ips` до `9.086ips`

### Находка №6
`allocated memory` говорит, что внутри цикла из нашей `Находки №4`
создается много объектов.

Наводим порядок и видем:
Метрика выросла с `9.086ips` до `10.744ips`

### Находка №7
В программе часто встречается навороченный `Array#count`.
Но ведь есть еще и `Array#size` и я предполагая, что он будет работать быстрее

Проверяем теорию
```
require 'benchmark/ips'

SIZE = 100_000
ARRAY = [*1..SIZE]

Benchmark.ips do |x|
  x.config(:stats => :bootstrap, :confidence => 99)

  x.report("Array#count")  { ARRAY.count }
  x.report("Array#size")   { ARRAY.size }
  x.report("Array#length") { ARRAY.length }

  x.compare!
end
```

И получаем вот такой отчет:
```
Calculating -------------------------------------
         Array#count      9.505M (± 2.4%) i/s -     46.235M in   5.005837s
          Array#size     11.231M (± 1.3%) i/s -     55.869M in   5.006641s
        Array#length     11.574M (± 0.6%) i/s -     57.867M in   5.006233s
                   with 99.0% confidence

Comparison:
        Array#length: 11573934.0 i/s
          Array#size: 11231052.5 i/s - 1.03x  (± 0.01) slower
         Array#count:  9504959.6 i/s - 1.22x  (± 0.03) slower
                   with 99.0% confidence
```
И понимаем, что лучше в программе заменить все на `Array#length`
Особого эффекта это конечно не дало, но все же это оптимизация

## Анализ асимптотики после оптимизации

```
Calculating -------------------------------------
    Process 0.0625Mb     55.788  (± 7.2%) i/s -    278.000  in   5.017200s
     Process 0.125Mb     23.655  (±16.9%) i/s -    114.000  in   5.001846s
      Process 0.25Mb     11.312  (± 8.8%) i/s -     57.000  in   5.085024s
       Process 0.5Mb      5.830  (± 0.0%) i/s -     30.000  in   5.159615s
         Process 1Mb      2.920  (± 0.0%) i/s -     15.000  in   5.181417s
         Process 2Mb      1.486  (± 0.0%) i/s -      8.000  in   5.396952s

Comparison:
    Process 0.0625Mb:       55.8 i/s
     Process 0.125Mb:       23.7 i/s - 2.36x   slower # x
      Process 0.25Mb:       11.3 i/s - 4.93x   slower # x * 2
       Process 0.5Mb:        5.8 i/s - 9.57x   slower # x * 1.9
         Process 1Mb:        2.9 i/s - 19.11x  slower # x * 2
         Process 2Mb:        1.5 i/s - 37.55x  slower # x * 1.96
```

Помоему замечательно, теперь при увеличении объёма исходных данных в два раза, время также замедляется в 2 раза!

## Результаты
Удалось улучшить метрику системы с `0.643ips` до `12.821ips` в 20 раз!
В результате проделанной оптимизации наконец удалось обработать файл с данными объемом 134.4Mb.
```
Calculating -------------------------------------
  Process 134.4 MB of data_large
    0.017  (± 0.0%) i/s - 1.000  in  58.717520s
```
