# Case-study оптимизации

## Актуальная проблема
В нашем проекте возникла серьёзная проблема.

Необходимо было обработать файл с данными, чуть больше ста мегабайт.

У нас уже была программа на `ruby`, которая умела делать нужную обработку.

Она успешно работала на файлах размером пару мегабайт, но для большого файла она работала слишком долго, и не было понятно, закончит ли она вообще работу за какое-то разумное время.

Я решил исправить эту проблему, оптимизировав эту программу.

## Гарантия корректности работы оптимизированной программы
Программа поставлялась с тестом. Выполнение этого теста позволяет не допустить изменения логики программы при оптимизации.

## Feedback-Loop
Для того, чтобы иметь возможность быстро проверять гипотезы я выстроил эффективный `feedback-loop`, который позволил мне получать
обратную связь по эффективности сделанных изменений за время, не более 30 сек.

Вот как я построил `feedback-loop`:
 * По скольку файл данных очень большой я обрезал его до 20к строк, получилось примерно 1Мб;
 * Время выполнения кода с обрезанным файлом составило примерно 30 секунд, что позволило быстрее получать обратную связь;
 * Для оценки времени использовал Benchmark;
 * Сразу после выполнения кода выполняется тест, чтобы можно было сразу увидеть работоспособность после изменений.

## Инструменты
Для того чтобы найти "точки роста" для оптимизации я воспользовался гемами:
 * memory_profiler
 * get_process_mem
 * ruby-prof

### Находка №1
`get_process_mem` показал, что больше всего потребление памяти уходит на цикл, 
где формируются массивы пользователей и сессий: с ~30 до ~230МБ. 
`memory_profiler` показал, что пик выделения памяти происхоит в 55 строке, где формируется массив сессий.
Дело в том что при каждой итерации создается много новых массивов.
`Array#push` снизило выделенную память примерно до 50Мб.

### Находка №2
`memory_profiler` показал, что в 101 строке, выбираются сессий через `#select`.
Мне показалось, что эфективнее будет `sessions` из файла записывать в `Hash` где key будет `id` пользователя, а value сами сессии.
После небольшого рефакторинга код теперь выполняется примерно за 0.5 сек.

### Находка №3
Еще `memory_profiler` показал большое кол-во объектов, на которые выделяется память в 
процессе работы кода. Total allocated: показал около 1кк объектов.
 * 103 строка: заменил конкатенацию на `Array#push`, теп самым лишних массивов стало меньше.
 * 53 строка: мы разбиваем строку на массив `line.split(',')` заменил `String#start_with?`; 
 * типы строк `user`, `session` вынес в константы.
по итогу объектов стало мельше в 1.5 раз и код выполняется за 0.4 сек

### Находка №4
`allocated memory`, топ занимает строка 140 где обрабатывается дата в сессях пользователя. 
Каждый из методов создает новые массивы в памяти.
`RubyProf` говорит, что много времени тратится на `Date#parse`. 
Заменил на `Date#strptime` получилось примерно в 2 раза быстрее.

### Находка №5
Далее 7 раз происходит вызов `collect_stats_from_users`.
все 7 вызовов в топ-10 `allocated memory`. Переделал код на `until` которые полностью собирает хеш.
Итог 0.3 сек

## Результаты
В результате проделанной оптимизации наконец удалось обработать файл с данными.
Удалось улучшить метрику системы с 30 секунд до 0.3 сек
